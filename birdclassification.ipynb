{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Bird Classification using a Weakly-Supervised Data Augmentation Network."},{"metadata":{},"cell_type":"markdown","source":"### Let's import some stuff to make this work"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport torchvision.models as models\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport random\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nimport torchvision.datasets as datasets\nfrom glob import glob\nimport cv2\nimport datetime as dt\nimport h5py\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\nimport pandas as pd\nimport os\ntorch.manual_seed(0)\nnames = pd.read_csv('../input/birds21wi/birds/names.txt', sep=\"\\n\", header=None)","execution_count":1,"outputs":[{"output_type":"stream","text":"cuda:0\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Preprocessing:\n\nFor preprocessing we get the images in both the train and test folder and compress them into hdf5 files. To do this we iterate through each image and resize them using OpenCV (to 128x128). Although in our network we use 299x299 images, using 128x128 in this stage proved to be efficient for loading the images into memory. Moreover, the use of HDF5 files allows us to increase the performance of our data laoders since they just have to look at memory instead of actual files in the system."},{"metadata":{"trusted":true},"cell_type":"code","source":"def proc_images(isTest):\n    \"\"\"\n    Saves compressed, resized images as HDF5 datsets\n    Returns\n        data.h5, where each dataset is an image or class label\n        e.g. X23,y23 = image and corresponding class label\n    \"\"\"\n    start = dt.datetime.now()\n    # ../input/\n    #PATH = os.path.abspath(os.path.join('..', 'input'))\n    # ../input/sample/images/\n    #SOURCE_IMAGES = os.path.join(PATH, \"sample\", \"images\")\n    # ../input/sample/images/*.png\n    if (isTest):\n        images = glob('../input/birds21wi/birds/test/**/*.jpg')\n        images.extend(glob('../input/birds21wi/birds/test/**/*.JPG'))\n    else:\n        images = glob('../input/birds21wi/birds/train/**/*.jpg')\n        images.extend(glob('../input/birds21wi/birds/train/**/*.JPG'))\n    # Load labels\n    labels = pd.read_csv('../input/birds21wi/birds/labels.csv')\n\n    # Set the disease type you want to look for\n\n    # Size of data\n    NUM_IMAGES = len(images)\n    HEIGHT = 128\n    WIDTH = 128\n    CHANNELS = 3\n    SHAPE = (HEIGHT, WIDTH, CHANNELS)\n\n    x_data = []\n    y_data = []\n    if isTest:\n        paths = []\n        fname = 'birds21hdf5_test.h5'\n    else:\n        fname = 'birds21hdf5.h5'\n    with h5py.File(fname, 'w') as hf: \n        for i,img in enumerate(images):            \n            # Images\n            image = cv2.imread(img)\n            image = cv2.resize(image, (WIDTH,HEIGHT), interpolation=cv2.INTER_CUBIC)\n            x_data.append(image)\n            # Labels\n            base = os.path.basename(img)\n            if isTest:\n                y_data.append('')\n                paths.append(base)\n            else:\n                finding = labels[\"class\"][labels[\"path\"] == base].values[0]\n                y_data.append(finding)\n            \n\n            end=dt.datetime.now()\n            print(\"\\r\", i, \": \", (end-start).seconds, \"seconds\", end=\"\")\n        xset = hf.create_dataset(  \n            name='images',\n            data=x_data,\n            shape=(NUM_IMAGES,HEIGHT, WIDTH, CHANNELS),\n            maxshape=(NUM_IMAGES, HEIGHT, WIDTH, CHANNELS),\n            compression=\"gzip\",\n            compression_opts=9)\n        if isTest:\n            paths = np.array(paths, dtype='S')\n            pathset = hf.create_dataset(\n                name='path',\n                data=paths,\n                shape=(1, NUM_IMAGES),\n                maxshape=(1, NUM_IMAGES)\n            )\n        else:\n            yset = hf.create_dataset(\n            name='labels',\n            data=y_data,\n            shape=(1, NUM_IMAGES),\n            maxshape=(1, NUM_IMAGES),\n            compression=\"gzip\",\n            compression_opts=9)\n#proc_images(True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data loader\nclass H5Dataset(torch.utils.data.Dataset):\n    def __init__(self, h5_file, transform=None):\n        self.transform = transform\n        self.h5_file = h5py.File(h5_file, 'r')\n        self.images = self.h5_file['images'][:]\n        self.labels = torch.LongTensor(self.h5_file['labels'][:]).transpose(0, 1)\n        self.labels = torch.flatten(self.labels)\n\n        \n    def __len__(self):\n        return self.labels.shape[0]\n      \n    def __getitem__(self, idx):\n        data = self.images[idx]\n        label = self.labels[idx]\n        \n        if self.transform:\n            data = self.transform(data)\n        return (data, label)\n#dataset = H5Dataset('data_2.h5')","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# H5Dataset for test dataset since we need information about the paths of the images when we make our final predictions\nclass H5DatasetTest(torch.utils.data.Dataset):\n    def __init__(self, h5_file, transform=None):\n        self.transform = transform\n        self.h5_file = h5py.File(h5_file, 'r')\n        self.images = self.h5_file['images'][:]\n        self.paths = np.array(self.h5_file['path'][:]).transpose()\n\n        \n    def __len__(self):\n        return self.paths.shape[0]\n      \n    def __getitem__(self, idx):\n        data = self.images[idx]\n        \n        if self.transform:\n            data = self.transform(data)\n        return (data, torch.tensor(idx))\n    \n    def target_to_path(self, target):\n        return str(self.paths[target.item()].item().decode('utf-8'))\n","execution_count":3,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To train our network we need some feedback on the accuracy it gets over data it has never seen. Therefore, we partition the images in the training folder so that we have roughly an 80, 20% split between the training data and test data (for our experiments, the actual test data is for the final predictions)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_birds21wi_data():\n    resize=(299, 299)\n    transform_train = transforms.Compose([\n        transforms.ToPILImage(),\n        transforms.Resize((299, 299)),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225])\n    ])\n\n    transform_test = transforms.Compose([\n        transforms.ToPILImage(),\n        transforms.Resize((299, 299)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225])\n    ])\n\n    dataset = H5Dataset('../input/birds21hdf5/birds21hdf5.h5')\n    #dataset = datasets.ImageFolder('../input/birds21wi/birds/train')\n    #print(len(dataset))\n    train_set, val_set = torch.utils.data.random_split(dataset, [30849, 7713])\n    train_set.dataset.transform = transform_train\n    val_set.dataset.transform = transform_test\n    trainloader = torch.utils.data.DataLoader(train_set, batch_size=20, shuffle=True, num_workers=8)\n    testloader = torch.utils.data.DataLoader(val_set, batch_size=20, shuffle=True, num_workers=8)\n    return {'train': trainloader, 'test':testloader}#, 'labels': dataset.class_to_idx}\ndata = get_birds21wi_data()","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_actual_label(output_label, class_to_idx):\n    key_list = list(class_to_idx.keys())\n    val_list = list(class_to_idx.values())\n    pos = val_list.index(output_label)\n    return int(key_list[pos])\ndata['train']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataiter = iter(data['train'])\nimages, labels = dataiter.next()\nimages = images[:8]\nprint(labels[0])\nprint(images.size())\n\ndef imshow(img):\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()\n\n# show images\n#print(data['train'].dataset.samples[i][0])\nimshow(images[0])\n# print labels\nprint(\"Labels:\" + ' '.join('%9s' % names.iloc[labels[j].item()] for j in range(1)))\n\nflat = torch.flatten(images, 1)\nprint(images.size())\nprint(flat.size())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Network\n\nHere we try to use a Weakly-Supervised Data Augmentation Network, where data augmentation is driven by attention maps rather than by randomness. In our WSDAN network we have a convolutional-neural network that outputs feature maps, another one outputs the attention maps from these features. Then the feature matrix we use for our predictions is outputed by a Bilinear Attention Pooling net. Our features are given by an inceptionv3 network that has been previously trained on the iNaturalist dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"EPSILON = 1e-12\n\n\nclass BasicConv2d(nn.Module):\n\n    def __init__(self, in_channels, out_channels, **kwargs):\n        super(BasicConv2d, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n        self.bn = nn.BatchNorm2d(out_channels, eps=0.001)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        return F.relu(x, inplace=True)\n\n# Bilinear Attention Pooling\nclass BAP(nn.Module):\n    def __init__(self, pool='GAP'):\n        super(BAP, self).__init__()\n        assert pool in ['GAP', 'GMP']\n        if pool == 'GAP':\n            self.pool = None\n        else:\n            self.pool = nn.AdaptiveMaxPool2d(1)\n\n    def forward(self, features, attentions):\n        B, C, H, W = features.size()\n        _, M, AH, AW = attentions.size()\n\n        # match size\n        if AH != H or AW != W:\n            attentions = F.interpolate(attentions, size=(H, W), mode='bilinear', align_corners=True)\n\n        # feature_matrix: (B, M, C) -> (B, M * C)\n        if self.pool is None:\n            feature_matrix = (torch.einsum('imjk,injk->imn', (attentions, features)) / float(H * W)).view(B, -1)\n        else:\n            feature_matrix = []\n            for i in range(M):\n                AiF = self.pool(features * attentions[:, i:i + 1, ...]).view(B, -1)\n                feature_matrix.append(AiF)\n            feature_matrix = torch.cat(feature_matrix, dim=1)\n\n        # sign-sqrt\n        feature_matrix = torch.sign(feature_matrix) * torch.sqrt(torch.abs(feature_matrix) + EPSILON)\n\n        # l2 normalization along dimension M and C\n        feature_matrix = F.normalize(feature_matrix, dim=-1)\n        return feature_matrix\n\n# WS-DAN Implementation: Weakly Supervised Data Augmentation Network for Fine-Grained\n# Visual Classification.\nclass WSDAN(nn.Module):\n    def __init__(self, M=32):\n        super(WSDAN, self).__init__()\n        self.num_classes = 555\n        self.M = M\n        \n        # Let's load the inceptio_v3 model untrained on ImageNet, instead we load\n        # the weights\n        model  = models.inception_v3(pretrained=False)\n        model.fc = nn.Linear(2048, 8142)\n        model.aux_logits = False\n        model.load_state_dict(torch.load('../input/v3-inat2018/iNat_2018_InceptionV3.pth.tar', map_location='cpu')['state_dict'])\n        model.eval()\n        \n\n        self.inception = nn.Sequential(\n            model.Conv2d_1a_3x3,\n            model.Conv2d_2a_3x3,\n            model.Conv2d_2b_3x3,\n            nn.MaxPool2d(kernel_size=3, stride=2),\n            model.Conv2d_3b_1x1,\n            model.Conv2d_4a_3x3,\n            nn.MaxPool2d(kernel_size=3, stride=2),\n            model.Mixed_5b,\n            model.Mixed_5c,\n            model.Mixed_5d,\n            model.Mixed_6a,\n            model.Mixed_6b,\n            model.Mixed_6c,\n            model.Mixed_6d,\n            model.Mixed_6e,\n            model.Mixed_7a,\n            model.Mixed_7b,\n            model.Mixed_7c,\n        )\n\n        self.num_features = 2048\n        \n        # attention maps\n        self.attentions = BasicConv2d(self.num_features, self.M, kernel_size=1)\n        \n        #Bilinear Attention Pooling\n        self.bap = BAP(pool='GAP')\n        \n        # Classification Layer\n        self.fc = nn.Linear(self.M * self.num_features, self.num_classes, bias=False)\n        \n    def forward(self, x, training):\n        batch_size = x.size(0)\n        feature_maps = self.inception(x)\n        \n        #attention_maps = self.attentions(feature_maps)\n\n        attention_maps = feature_maps[:, :self.M, ...]\n        feature_matrix = self.bap(feature_maps, attention_maps)\n        \n        # Classification\n        p = self.fc(feature_matrix * 100.)\n\n        # Generate Attention Map\n        if training:\n            # Randomly choose one of attention maps Ak\n            attention_map = []\n            for i in range(batch_size):\n                attention_weights = torch.sqrt(attention_maps[i].sum(dim=(1, 2)).detach() + EPSILON)\n                attention_weights = F.normalize(attention_weights, p=1, dim=0)\n                k_index = np.random.choice(self.M, 2, p=attention_weights.cpu().numpy())\n                attention_map.append(attention_maps[i, k_index, ...])\n            attention_map = torch.stack(attention_map)  # (B, 2, H, W) - one for cropping, the other for dropping\n        else:\n            # Object Localization Am = mean(Ak)\n            attention_map = torch.mean(attention_maps, dim=1, keepdim=True)  # (B, 1, H, W)\n\n        # p: (B, self.num_classes)\n        # feature_matrix: (B, M * C)\n        # attention_map: (B, 2, H, W) in training, (B, 1, H, W) in val/testing\n        return p, feature_matrix, attention_map\n#net=WSDAN()        ","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##################################\n# augment function\n##################################\ndef batch_augment(images, attention_map, mode='crop', theta=0.5, padding_ratio=0.1):\n    batches, _, imgH, imgW = images.size()\n\n    if mode == 'crop':\n        crop_images = []\n        for batch_index in range(batches):\n    \n            atten_map = attention_map[batch_index:batch_index + 1]\n            if isinstance(theta, tuple):\n                theta_c = random.uniform(*theta) * atten_map.max()\n            else:\n                theta_c = theta * atten_map.max()\n\n            crop_mask = F.interpolate(atten_map, size=(imgH, imgW), mode='bilinear', align_corners=True) >= theta_c\n            nonzero_indices = torch.nonzero(crop_mask[0, 0, ...])\n            height_min = max(int(nonzero_indices[:, 0].min().item() - padding_ratio * imgH), 0)\n            height_max = min(int(nonzero_indices[:, 0].max().item() + padding_ratio * imgH), imgH)\n            width_min = max(int(nonzero_indices[:, 1].min().item() - padding_ratio * imgW), 0)\n            width_max = min(int(nonzero_indices[:, 1].max().item() + padding_ratio * imgW), imgW)\n\n            crop_images.append(\n                F.interpolate(images[batch_index:batch_index + 1, :, height_min:height_max, width_min:width_max],\n                                    size=(imgH, imgW), mode='bilinear', align_corners=True))\n        crop_images = torch.cat(crop_images, dim=0)\n        return crop_images\n\n    elif mode == 'drop':\n        drop_masks = []\n        for batch_index in range(batches):\n            atten_map = attention_map[batch_index:batch_index + 1]\n            if isinstance(theta, tuple):\n                theta_d = random.uniform(*theta) * atten_map.max()\n            else:\n                theta_d = theta * atten_map.max()\n\n            drop_masks.append(F.interpolate(atten_map, size=(imgH, imgW), mode='bilinear', align_corners=True) < theta_d)\n        drop_masks = torch.cat(drop_masks, dim=0)\n        drop_images = images * drop_masks.float()\n        return drop_images\n    \n\n##############################################\n# Center Loss for Attention Regularization\n##############################################\nclass CenterLoss(nn.Module):\n    def __init__(self):\n        super(CenterLoss, self).__init__()\n        self.l2_loss = nn.MSELoss(reduction='sum')\n\n    def forward(self, outputs, targets):\n        return self.l2_loss(outputs, targets) / outputs.size(0)\n\ndef generate_heatmap(attention_maps):\n    heat_attention_maps = []\n    heat_attention_maps.append(attention_maps[:, 0, ...])  # R\n    heat_attention_maps.append(attention_maps[:, 0, ...] * (attention_maps[:, 0, ...] < 0.5).float() + \\\n                               (1. - attention_maps[:, 0, ...]) * (attention_maps[:, 0, ...] >= 0.5).float())  # G\n    heat_attention_maps.append(1. - attention_maps[:, 0, ...])  # B\n    return torch.stack(heat_attention_maps, dim=1)\n\nToPILImage = transforms.ToPILImage()\nMEAN = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)\nSTD = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)","execution_count":7,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training\n\nThe training function is similar to past versions with some updates. In particular, now there is a `schedule` parameter to handle learning rate scheduling and also a `checkpoint_path` parameter which will be where training checkpoints are saved (if provided).\n\nThe core of training is the same though, get a batch, run the model forward, calculate loss, run it backward, update."},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(net, dataloader, optimizer=None, epochs=1, start_epoch=0, lr=0.01, momentum=0.9, decay=0.0005, \n          verbose=1, print_every=10, state=None, scheduler=None, schedule={}, checkpoint_path=None, beta=5e-2):\n    net = net.to(device)\n    net.train()\n    losses = []\n    criterion = nn.CrossEntropyLoss()\n    center_loss = CenterLoss()\n    train_accuracies = []\n    test_accuracies = []\n    num_classes = 555\n    optimizer = optim.SGD(net.parameters(), lr=lr, momentum=momentum, weight_decay=decay)\n    \n    # feature_center: size of (#classes, #attention_maps * #channel_features)\n    feature_center = torch.zeros(num_classes, 32 * net.num_features).to(device)\n    \n    # Load previous training state\n    if state:\n        net.load_state_dict(state['net'])\n        optimizer.load_state_dict(state['optimizer'])\n        start_epoch = state['epoch']\n        losses = state['losses']\n        feature_center = state['feature_center']\n\n  # Fast forward lr schedule through already trained epochs\n    for epoch in range(start_epoch):\n        if epoch in schedule:\n            print (\"Learning rate: %f\"% schedule[epoch])\n            for g in optimizer.param_groups:\n                g['lr'] = schedule[epoch]\n\n    for epoch in range(start_epoch, epochs):\n        sum_loss = 0.0\n\n    # Update learning rate when scheduled\n        if epoch in schedule:\n            print(\"entering schedule change\\n\")\n            print (\"Learning rate: %f\"% schedule[epoch])\n            for g in optimizer.param_groups:\n                g['lr'] = schedule[epoch]\n\n        for i, batch in enumerate(dataloader, 0):\n            inputs, labels = batch[0].to(device), batch[1].to(device)\n\n            optimizer.zero_grad()\n            \n            # Raw Image. i.e., no cropping or dropping, the original input image\n\n            output_raw, feature_matrix, attention_map = net(inputs, True)\n            \n            # Update Feature Center\n            feature_center_batch = F.normalize(feature_center[labels], dim=-1)\n            feature_center[labels] += beta * (feature_matrix.detach() - feature_center_batch)\n            \n            ##################################\n            # Attention Cropping\n            ##################################\n            with torch.no_grad():\n                crop_images = batch_augment(inputs, attention_map[:, :1, :, :], mode='crop', theta=(0.4, 0.6), padding_ratio=0.1)\n                \n            output_crop, _, _ = net(crop_images, True)\n            \n            ##################################\n            # Attention Dropping\n            ##################################\n            with torch.no_grad():\n                drop_images = batch_augment(inputs, attention_map[:, 1:, :, :], mode='drop', theta=(0.2, 0.5))\n            \n            output_drop, _, _ = net(drop_images, True)\n\n            loss = criterion(output_raw, labels) / 3. + \\\n             criterion(output_crop, labels) / 3. + \\\n             criterion(output_drop, labels) / 3. + \\\n             center_loss(feature_matrix, feature_center_batch)\n            \n            loss.backward()  # autograd magic, computes all the partial derivatives\n            optimizer.step() # takes a step in gradient direction\n\n            losses.append(loss.item())\n            sum_loss += loss.item()\n            if scheduler:\n                scheduler.step()\n            if i % print_every == print_every-1:    # print every 10 mini-batches\n                if verbose:\n                    print('[%d, %5d] loss: %.3f' % (epoch, i + 1, sum_loss / print_every))\n                sum_loss = 0.0\n        test_accuracies.append(accuracy(net, data['test']))\n        print(test_accuracies[-1])\n        net.train()\n        if checkpoint_path and (((epoch + 1) % 5 == 0) or (epoch == 0)):\n            print(\"reached checkpoint\")\n            state = {'epoch': epoch+1, 'net': net.state_dict(), 'optimizer': optimizer.state_dict(), 'losses': losses, 'feature_center':feature_center}\n            torch.save(state, 'checkpoint-%d.pkl'%(epoch+1))\n        print(\"Finished epoch\")\n    return losses, test_accuracies\n\ndef accuracy(net, dataloader):\n    net = net.to(device)\n    net.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for batch in dataloader:\n            images, labels = batch[0].to(device), batch[1].to(device)\n            output_raw, _, attention_maps = net(images, False)\n            \n            crop_images = batch_augment(images, attention_maps, mode='crop', theta=0.1, padding_ratio=0.05)\n            output_crop, _, _ = net(crop_images, False)\n            \n            outputs = (output_raw + output_crop) / 2.\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    return correct/total\n\ndef smooth(x, size):\n    return np.convolve(x, np.ones(size)/size, mode='valid')","execution_count":14,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's try finetuning the entire model using the pretrained weights. Then we will try it as a feature extractor."},{"metadata":{"trusted":true},"cell_type":"code","source":"\nmodel = WSDAN()\npretrain_losses, accuracies = train(model, data['train'],start_epoch=10, epochs=40, \n                                    state=torch.load('checkpoint-10.pkl'),\n                                    schedule={0:0.001, 5:0.0001, 10:0.00001, 20:0.000001},\n                                    print_every=100, checkpoint_path='lol')\nprint(\"Testing accuracy: %f\" % accuracy(model, data['test']))\nplt.plot(smooth(pretrain_losses,50))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transform_act_test = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.Resize((299, 299)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                             std=[0.229, 0.224, 0.225])\n])\ntest_dataset = H5DatasetTest('../input/birds21-train/birds21hdf5_test.h5', transform=transform_act_test)\ntestloader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=8)\ndef predict(net, dataloader, ofname):\n    out = open(ofname, 'w')\n    out.write(\"path,class\\n\")\n    net.to(device)\n    net.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for i, (images, labels) in enumerate(dataloader, 0):\n            if i%100 == 0:\n                print(i)\n            images, labels = images.to(device), labels.to(device)\n            output_raw, _, attention_maps = net(images, False)\n            \n            crop_images = batch_augment(images, attention_maps, mode='crop', theta=0.1, padding_ratio=0.05)\n            output_crop, _, _ = net(crop_images, False)\n            \n            outputs = (output_raw + output_crop) / 2.\n            _, predicted = torch.max(outputs.data, 1)\n            #fname, _ = dataloader.dataset.samples[i]\n            #print(labels)\n            path = test_dataset.target_to_path(labels)\n            #print(labels)\n            out.write(\"test/{},{}\\n\".format(path.split('/')[-1], predicted.item()))\n            #break\n    out.close()\npredict(model, testloader, \"preds.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}